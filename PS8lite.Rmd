---
title: "Random Forests"
date: "Last updated on `r Sys.Date()`"
output:
  html_document: 
    # code_folding: show
    df_print: kable
    highlight: tango
    number_sections: yes
    theme: cosmo
    toc: yes
    toc_depth: 2
    toc_float:
      collapsed: yes
---

```{r setup, include=FALSE}
# Set default behavior for all code chunks here:
knitr::opts_chunk$set(
  echo = TRUE, 
  message = FALSE, 
  warning = FALSE, 
  fig.width = 16/2, 
  fig.height = 9/2
)

# Load all your used packages here:
library(tidyverse)
library(janitor)
library(skimr)
library(randomForest)
library(caret)

# Set seed value of random number generator here:
set.seed(76)

# Load data 
training <- read_csv("data/train.csv")
test <- read_csv("data/test.csv")
sample_submission <- read_csv("data/sample_submission.csv")
```


***



# Preprocessing data

We do some variable cleaning to `training` and `test`. Note how for each step, we apply the changes for both the training and test sets. This is an important principle in machine learning: the training set must be representative of the test set.


## Clean variable names

Rename variables that start with a number, as such variable names can be problematic in R. 

```{r}
training <- training %>% 
  rename(
    FirstFlrSF = `1stFlrSF`,
    SecondFlrSF = `2ndFlrSF`,
    ThirdSsnPorch = `3SsnPorch`
  )

test <- test %>% 
  rename(
    FirstFlrSF = `1stFlrSF`,
    SecondFlrSF = `2ndFlrSF`,
    ThirdSsnPorch = `3SsnPorch`
  )
```


## Create new outcome variable

We are going to fit our models in log-space for many reasons:

1. To avoid situations where we obtain negative predicted values
1. To unskew the highly right-skewed original outcome variable `SalePrice`
1. The Kaggle score is RMSLE and not RMSE. So the following are roughly equivalent
1. "Fitting models to $y$ = `SalePrice` and using RMSLE as the score"
1. "Fitting models to $y$ = `logSalePrice` and using RMSE as the score"

```{r}
training <- training %>% 
  mutate(logSalePrice = log(SalePrice+1))
```


If you note, this transformation is only done to the `training` set, this is because our `test` set does not have the outcome variabe SalePrice. This is what we are trying to predict with our model. We are transforming the outcome variable above for the `training` set and it is important to make sure that we undo this transformation later on after we fit our model (i.e when we are predicting). 


## Select only numerical predictors

To keep things simple, we're only going to focus on the 36 numerical predictor variables. Given this fact, it's good idea to select only the variables we are going to use. 

```{r}
training <- training %>% 
  select(
    # Important non-predictor variables
    Id, SalePrice, logSalePrice,
    # All numerical predictor variables
    MSSubClass, LotFrontage, LotArea, OverallQual, OverallCond, YearBuilt, YearRemodAdd, 
    MasVnrArea, BsmtFinSF1, BsmtFinSF2, BsmtUnfSF, TotalBsmtSF, FirstFlrSF, SecondFlrSF, 
    LowQualFinSF, GrLivArea, BsmtFullBath, BsmtHalfBath, FullBath, HalfBath, BedroomAbvGr,
    KitchenAbvGr, TotRmsAbvGrd, Fireplaces, GarageYrBlt, GarageCars, GarageArea, WoodDeckSF, 
    OpenPorchSF, EnclosedPorch, ThirdSsnPorch, ScreenPorch, PoolArea, MiscVal, MoSold, YrSold
  )

test <- test %>% 
  select(
    # Important non-predictor variables
    Id,
    # All numerical predictor variables
    MSSubClass, LotFrontage, LotArea, OverallQual, OverallCond, YearBuilt, YearRemodAdd, 
    MasVnrArea, BsmtFinSF1, BsmtFinSF2, BsmtUnfSF, TotalBsmtSF, FirstFlrSF, SecondFlrSF, 
    LowQualFinSF, GrLivArea, BsmtFullBath, BsmtHalfBath, FullBath, HalfBath, BedroomAbvGr,
    KitchenAbvGr, TotRmsAbvGrd, Fireplaces, GarageYrBlt, GarageCars, GarageArea, WoodDeckSF, 
    OpenPorchSF, EnclosedPorch, ThirdSsnPorch, ScreenPorch, PoolArea, MiscVal, MoSold, YrSold
  )
```


## Deal with missing values

Many of these numerical predictors have missing values.

```{r, eval=FALSE}
skim(training)
skim(test)
```

An MVP approach to dealing with them is to replace them with the mean of the non-missing values". Note: there must be a better way to do this, in particular using the `purrr::map()` function, but done is better than perfect.

```{r}
training <- training %>% 
  mutate(
    LotFrontage = ifelse(is.na(LotFrontage), mean(LotFrontage, na.rm = TRUE), LotFrontage),
    MasVnrArea = ifelse(is.na(MasVnrArea), mean(MasVnrArea, na.rm = TRUE), MasVnrArea),
    GarageYrBlt = ifelse(is.na(GarageYrBlt), mean(GarageYrBlt, na.rm = TRUE), GarageYrBlt)
  )
test <- test %>% 
  mutate(
    LotFrontage = ifelse(is.na(LotFrontage), mean(LotFrontage, na.rm = TRUE), LotFrontage),
    MasVnrArea = ifelse(is.na(MasVnrArea), mean(MasVnrArea, na.rm = TRUE), MasVnrArea),
    BsmtFinSF1 = ifelse(is.na(BsmtFinSF1), mean(BsmtFinSF1, na.rm = TRUE), BsmtFinSF1),
    BsmtFinSF2 = ifelse(is.na(BsmtFinSF2), mean(BsmtFinSF2, na.rm = TRUE), BsmtFinSF2),
    BsmtUnfSF = ifelse(is.na(BsmtUnfSF), mean(BsmtUnfSF, na.rm = TRUE), BsmtUnfSF),
    TotalBsmtSF = ifelse(is.na(TotalBsmtSF), mean(TotalBsmtSF, na.rm = TRUE), TotalBsmtSF),
    BsmtFullBath = ifelse(is.na(BsmtFullBath), mean(BsmtFullBath, na.rm = TRUE), BsmtFullBath),
    BsmtHalfBath = ifelse(is.na(BsmtHalfBath), mean(BsmtHalfBath, na.rm = TRUE), BsmtHalfBath),
    GarageYrBlt = ifelse(is.na(GarageYrBlt), mean(GarageYrBlt, na.rm = TRUE), GarageYrBlt),    
    GarageCars = ifelse(is.na(GarageCars), mean(GarageCars, na.rm = TRUE), GarageCars),
    GarageArea = ifelse(is.na(GarageArea), mean(GarageArea, na.rm = TRUE), GarageArea)
  )
```


## Define model formula

We use the same model formula as the `model_formula_full` from PS7. In other words, we are using the same $p$ = 36 numerical predictors.

```{r}
model_formula <- "logSalePrice ~ MSSubClass + LotFrontage + LotArea + 
OverallQual + OverallCond + YearBuilt + YearRemodAdd + MasVnrArea + BsmtFinSF1 + 
BsmtFinSF2 + BsmtUnfSF + TotalBsmtSF + FirstFlrSF + SecondFlrSF + LowQualFinSF + 
GrLivArea + BsmtFullBath + BsmtHalfBath + FullBath + HalfBath + BedroomAbvGr + 
KitchenAbvGr + TotRmsAbvGrd + Fireplaces + GarageYrBlt + GarageCars + GarageArea + 
WoodDeckSF + OpenPorchSF + EnclosedPorch + ThirdSsnPorch + ScreenPorch + PoolArea + 
MiscVal + MoSold + YrSold" %>% 
  as.formula()
```



***

# Fitting a `randomForest` via `caret`


## Setup cross-validation

We're going use `caret` package functionality to

1. Define cross-validation settings. We are going to use cross-validation to generate model error estimates (when using fitted models to make predictions on new independent test data)
1. Setup a "search grid" of "tuning" parameters to find the "optimal value." By "optimize" we mean pick the value of the "tuning" parameter that yields the lowest cross-validated error estimate.

The latter is like how in the:

1. CART problem set you searched over a range of $\alpha$ values
1. LASSO problem set you searched over a range of $\lambda$ values

The "tuning" parameter we're going to "optimize" over is the number of randomly chosen variables we use at each split in our CART trees: `mtry`. We're going to search over values 2 through 5 (the maximum number of predictor variables we have access to). 

```{r}
# Define cross-validation settings: 10-fold CV
fit_control <- trainControl(method = "cv", number = 10)

# Setup search grid of "tuning" parameters
mtry <- 2:10
tunegrid <- expand.grid(.mtry = mtry)
```


## Perform cross-validation

Note that this code chunk takes a few minutes to run. So we set this code chucks `cache = TRUE` to save the result when knitting the .Rmd file, so that future knits don't re-run this code block to save time. 

```{r, cache = TRUE}
model_rf_caret <- caret::train(
  # Model formula
  form = model_formula,
  # Training data
  data = training, 
  # Set method to randomForests. Note: this is where you can switch out to
  # different methods
  method = "rf",
  # Score/error metric used:
  metric = "RMSE",
  # Cross-validation settings:
  trControl = fit_control,
  # Search grid of tuning parameters
  tuneGrid = tunegrid
  )
```

Let's study the output:

```{r}
model_rf_caret
```


## Predict on test data

Note how we use the `predict()` function to generate predicted values of `logSalePrice_hat`. By default, the `predict()` function will use the optimal value of `mtry` of 10 which has the lowest RMSE score.

```{r}
test <- test %>% 
  mutate(
    logSalePrice_hat_rf_caret = predict(model_rf_caret, test),
    SalePrice_hat_rf_caret = exp(logSalePrice_hat_rf_caret) -1
  )

```

Optionally, create a Kaggle submission.

```{r, eval=FALSE}
submission_rf_caret <- test %>%
  mutate(SalePrice = SalePrice_hat_rf_caret)%>%
  select(Id, SalePrice) 

write_csv(submission_rf_caret, "/Users/lorti/Desktop/SDS 293/PS8lite/data/submission_rf_caret.csv")
```


![](images/score_screenshot_caret.png){ width=100% }
